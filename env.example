# IdiomApp Configuration Example
# Copy this file to .env and modify as needed

# General configuration
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL

# LLM Provider Configuration
# Choose between "ollama" or "openai"
LLM_PROVIDER=ollama

# Ollama configuration (if using Ollama)
# Use http://localhost:11434 for local dev, http://ollama:11434 for Docker
OLLAMA_HOST=http://ollama:11434
DEFAULT_MODEL=llama3.2:latest

# Available Ollama models (comma-separated list)
OLLAMA_MODELS=llama3.2:latest,llama3:latest,mistral:latest

# OpenAI configuration (if using OpenAI)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ORGANIZATION=your_openai_organization_id_here
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.7  # Controls randomness (0.0-2.0)
OPENAI_MAX_TOKENS=1024  # Maximum tokens in response

# Streamlit configuration
STREAMLIT_SERVER_PORT=8503
# Use false for local dev, true for Docker/production
STREAMLIT_SERVER_HEADLESS=true
STREAMLIT_SERVER_ENABLECORS=false
STREAMLIT_SERVER_ENABLEXSRFPROTECTION=true
# Use localhost for local dev, 0.0.0.0 for Docker
STREAMLIT_SERVER_ADDRESS=0.0.0.0
STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
STREAMLIT_UI_HIDE_SIDEBAR_NAV=true
STREAMLIT_THEME_BASE=dark
STREAMLIT_BROWSER_SERVER_ADDRESS=localhost
STREAMLIT_CLIENT_TOOLBAR_MODE=minimal
STREAMLIT_CLIENT_SHOW_ERROR_DETAILS=true
STREAMLIT_WIDE_MODE=true

# Language options
SUPPORTED_LANGUAGES=en,es,ca
DEFAULT_SOURCE_LANGUAGE=en
DEFAULT_TARGET_LANGUAGES=es,ca

# Co-occurrence network defaults
DEFAULT_WINDOW_SIZE=2
DEFAULT_MIN_FREQUENCY=1
DEFAULT_POS_FILTER=NOUN,VERB,ADJ 